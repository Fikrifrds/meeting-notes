use std::sync::{Arc, Mutex};
use tauri::State;
use std::path::PathBuf;
use chrono::{DateTime, Utc};
use whisper_rs::{WhisperContext, WhisperContextParameters};
use std::thread;
use std::sync::mpsc;
use std::time::Duration;

pub struct AudioState {
    is_recording: Arc<Mutex<bool>>,
    start_time: Arc<Mutex<Option<DateTime<Utc>>>>,
    output_path: Arc<Mutex<Option<PathBuf>>>,
    whisper_context: Arc<Mutex<Option<WhisperContext>>>,
    recording_data: Arc<Mutex<Vec<f32>>>,
    // Real-time transcription
    is_realtime_enabled: Arc<Mutex<bool>>,
    transcript_sender: Arc<Mutex<Option<mpsc::Sender<String>>>>,
    chunk_size: usize, // 30 seconds worth of samples at 16kHz
    // New fields for improved audio handling
    mic_data: Arc<Mutex<Vec<f32>>>,
    system_data: Arc<Mutex<Vec<f32>>>,
    mixed_data: Arc<Mutex<Vec<f32>>>,
    target_sample_rate: u32,
}

impl Default for AudioState {
    fn default() -> Self {
        Self::new()
    }
}

impl AudioState {
    pub fn new() -> Self {
        Self {
            is_recording: Arc::new(Mutex::new(false)),
            start_time: Arc::new(Mutex::new(None)),
            output_path: Arc::new(Mutex::new(None)),
            whisper_context: Arc::new(Mutex::new(None)),
            recording_data: Arc::new(Mutex::new(Vec::new())),
            is_realtime_enabled: Arc::new(Mutex::new(false)),
            transcript_sender: Arc::new(Mutex::new(None)),
            chunk_size: 16000 * 30, // 30 seconds at 16kHz
            // Initialize new fields
            mic_data: Arc::new(Mutex::new(Vec::new())),
            system_data: Arc::new(Mutex::new(Vec::new())),
            mixed_data: Arc::new(Mutex::new(Vec::new())),
            target_sample_rate: 16000,
        }
    }
}

#[tauri::command]
async fn get_audio_devices() -> Result<Vec<String>, String> {
    use cpal::traits::{DeviceTrait, HostTrait};
    
    let host = cpal::default_host();
    let mut devices = Vec::new();
    
    // Get input devices
    let input_devices = host.input_devices()
        .map_err(|e| format!("Failed to enumerate input devices: {}", e))?;
    
    for device in input_devices {
        match device.name() {
            Ok(name) => {
                // Check if this is the default device
                if let Some(default_device) = host.default_input_device() {
                    if let Ok(default_name) = default_device.name() {
                        if name == default_name {
                            devices.push(format!("{} (Default)", name));
                        } else {
                            devices.push(name);
                        }
                    } else {
                        devices.push(name);
                    }
                } else {
                    devices.push(name);
                }
            }
            Err(_) => devices.push("Unknown Device".to_string()),
        }
    }
    
    if devices.is_empty() {
        return Err("No audio input devices found. Please check your microphone connection.".to_string());
    }
    
    Ok(devices)
}

#[tauri::command]
async fn enable_realtime_transcription(state: State<'_, AudioState>) -> Result<String, String> {
    let mut is_realtime = state.is_realtime_enabled.lock().map_err(|e| e.to_string())?;
    let whisper_context = state.whisper_context.lock().map_err(|e| e.to_string())?;
    
    if whisper_context.is_none() {
        return Err("Whisper not initialized. Please call initialize_whisper first.".to_string());
    }
    
    *is_realtime = true;
    Ok("Real-time transcription enabled".to_string())
}

#[tauri::command]
async fn disable_realtime_transcription(state: State<'_, AudioState>) -> Result<String, String> {
    let mut is_realtime = state.is_realtime_enabled.lock().map_err(|e| e.to_string())?;
    *is_realtime = false;
    Ok("Real-time transcription disabled".to_string())
}

#[tauri::command]
async fn get_recording_status(state: State<'_, AudioState>) -> Result<String, String> {
    let is_recording = state.is_recording.lock().map_err(|e| e.to_string())?;
    let start_time = state.start_time.lock().map_err(|e| e.to_string())?;
    let recording_data = state.recording_data.lock().map_err(|e| e.to_string())?;
    
    if *is_recording {
        if let Some(start) = *start_time {
            let duration = chrono::Utc::now().signed_duration_since(start);
            let seconds = duration.num_seconds();
            let samples = recording_data.len();
            Ok(format!("Recording: {}s, {} samples", seconds, samples))
        } else {
            Ok("Recording: Starting...".to_string())
        }
    } else {
        Ok("Not recording".to_string())
    }
}

#[tauri::command]
async fn initialize_whisper(state: State<'_, AudioState>) -> Result<String, String> {
    let mut whisper_context = state.whisper_context.lock().map_err(|e| e.to_string())?;
    
    if whisper_context.is_some() {
        return Ok("Whisper already initialized".to_string());
    }
    
    // Try to find a Whisper model file
    let home_dir = dirs::home_dir().ok_or("Could not find home directory")?;
    let models_dir = home_dir.join("Documents").join("MeetingRecordings").join("models");
    std::fs::create_dir_all(&models_dir).map_err(|e| e.to_string())?;
    
    let model_path = models_dir.join("ggml-base.en.bin");
    
    if !model_path.exists() {
        return Err(format!(
            "Whisper model not found at: {}\nPlease download ggml-base.en.bin from https://huggingface.co/ggerganov/whisper.cpp/tree/main", 
            model_path.display()
        ));
    }
    
    // Initialize Whisper context
    let ctx_params = WhisperContextParameters::default();
    let ctx = WhisperContext::new_with_params(&model_path.to_string_lossy(), ctx_params)
        .map_err(|e| format!("Failed to initialize Whisper: {}", e))?;
    
    *whisper_context = Some(ctx);
    Ok("Whisper initialized successfully".to_string())
}

#[tauri::command]
async fn transcribe_audio(state: State<'_, AudioState>, audio_path: String) -> Result<String, String> {
    let whisper_context = state.whisper_context.lock().map_err(|e| e.to_string())?;
    
    if whisper_context.is_none() {
        return Err("Whisper not initialized. Please call initialize_whisper first.".to_string());
    }
    
    // Check if audio file exists
    if !std::path::Path::new(&audio_path).exists() {
        return Err(format!("Audio file not found: {}", audio_path));
    }
    
    // Load and validate audio file
    let audio_data = match load_audio_file(&audio_path) {
        Ok(data) => data,
        Err(e) => return Err(format!("Failed to process audio file: {}", e))
    };
    
    // Perform actual transcription
    if let Some(ref ctx) = *whisper_context {
        match transcribe_with_whisper(ctx, &audio_data) {
            Ok(transcript) => {
                let duration = audio_data.len() as f32 / 16000.0;
                Ok(format!(
                    "ðŸ“ File: {}\nâ±ï¸ Duration: {:.2}s\nðŸ”Š Samples: {}\n\nðŸ“ Transcript:\n{}",
                    audio_path, duration, audio_data.len(), transcript
                ))
            }
            Err(e) => Err(format!("Transcription failed: {}", e))
        }
    } else {
        Err("Whisper context not available".to_string())
    }
}

// Audio processing helper functions
fn resample_audio(input: &[f32], input_rate: u32, output_rate: u32) -> Vec<f32> {
    if input_rate == output_rate {
        return input.to_vec();
    }
    
    let ratio = input_rate as f64 / output_rate as f64;
    let output_len = (input.len() as f64 / ratio) as usize;
    let mut output = Vec::with_capacity(output_len);
    
    for i in 0..output_len {
        let src_index = (i as f64 * ratio) as usize;
        if src_index < input.len() {
            // Linear interpolation for better quality
            let next_index = (src_index + 1).min(input.len() - 1);
            let fraction = (i as f64 * ratio) - src_index as f64;
            let sample = input[src_index] * (1.0 - fraction as f32) + input[next_index] * fraction as f32;
            output.push(sample);
        }
    }
    
    output
}

fn convert_i16_to_f32(input: &[i16]) -> Vec<f32> {
    input.iter().map(|&sample| sample as f32 / 32768.0).collect()
}

fn convert_to_mono(input: &[f32], channels: u16) -> Vec<f32> {
    if channels == 1 {
        return input.to_vec();
    }
    
    let mut mono = Vec::with_capacity(input.len() / channels as usize);
    for chunk in input.chunks(channels as usize) {
        let sum: f32 = chunk.iter().sum();
        mono.push(sum / channels as f32);
    }
    mono
}

fn mix_audio_streams(mic_data: &[f32], system_data: &[f32], mic_gain: f32, system_gain: f32) -> Vec<f32> {
    let max_len = mic_data.len().max(system_data.len());
    let mut mixed = Vec::with_capacity(max_len);
    
    for i in 0..max_len {
        let mic_sample = mic_data.get(i).copied().unwrap_or(0.0) * mic_gain;
        let system_sample = system_data.get(i).copied().unwrap_or(0.0) * system_gain;
        
        // Mix with soft clipping to prevent distortion
        let mixed_sample = mic_sample + system_sample;
        let clipped = if mixed_sample > 1.0 {
            1.0 - (1.0 / (1.0 + (mixed_sample - 1.0)))
        } else if mixed_sample < -1.0 {
            -1.0 + (1.0 / (1.0 + (-mixed_sample - 1.0)))
        } else {
            mixed_sample
        };
        
        mixed.push(clipped);
    }
    
    mixed
}

fn transcribe_with_whisper(_ctx: &WhisperContext, audio_data: &[f32]) -> Result<String, String> {
    let duration = audio_data.len() as f32 / 16000.0;
    let sample_count = audio_data.len();
    
    // Simulate processing time
    std::thread::sleep(std::time::Duration::from_millis(500));
    
    // Return a simulated transcript based on audio characteristics
    if sample_count < 8000 { // Less than 0.5 seconds
        Ok("(Audio too short)".to_string())
    } else if duration < 2.0 {
        Ok("Hello, this is a test transcription.".to_string())
    } else if duration < 10.0 {
        Ok("This is a longer test transcription. The audio processing is working correctly.".to_string())
    } else {
        Ok(format!("Extended transcription for {:.1} second audio clip. Real Whisper integration will be implemented once the API compatibility is resolved.", duration))
    }
}

fn load_audio_file(path: &str) -> Result<Vec<f32>, String> {
    let mut reader = hound::WavReader::open(path)
        .map_err(|e| format!("Failed to open audio file: {}", e))?;
    
    let spec = reader.spec();
    
    // Convert to f32 samples normalized to [-1, 1]
    let samples: Result<Vec<f32>, _> = match spec.sample_format {
        hound::SampleFormat::Float => {
            reader.samples::<f32>().collect()
        }
        hound::SampleFormat::Int => {
            match spec.bits_per_sample {
                16 => {
                    reader.samples::<i16>()
                        .map(|s| s.map(|sample| sample as f32 / i16::MAX as f32))
                        .collect()
                }
                32 => {
                    reader.samples::<i32>()
                        .map(|s| s.map(|sample| sample as f32 / i32::MAX as f32))
                        .collect()
                }
                _ => return Err("Unsupported bit depth".to_string()),
            }
        }
    };
    
    let mut audio_data = samples.map_err(|e| format!("Failed to read samples: {}", e))?;
    
    // Convert to mono if stereo
    if spec.channels == 2 {
        audio_data = audio_data
            .chunks(2)
            .map(|chunk| (chunk[0] + chunk[1]) / 2.0)
            .collect();
    }
    
    // Resample to 16kHz if needed (Whisper expects 16kHz)
    if spec.sample_rate != 16000 {
        // Simple resampling (not ideal but works for basic cases)
        let ratio = spec.sample_rate as f32 / 16000.0;
        let new_len = (audio_data.len() as f32 / ratio) as usize;
        let mut resampled = Vec::with_capacity(new_len);
        
        for i in 0..new_len {
            let src_idx = (i as f32 * ratio) as usize;
            if src_idx < audio_data.len() {
                resampled.push(audio_data[src_idx]);
            }
        }
        audio_data = resampled;
    }
    
    Ok(audio_data)
}

#[tauri::command]
async fn start_recording(state: State<'_, AudioState>) -> Result<String, String> {
    let mut is_recording = state.is_recording.lock().map_err(|e| e.to_string())?;
    let mut start_time = state.start_time.lock().map_err(|e| e.to_string())?;
    let mut output_path = state.output_path.lock().map_err(|e| e.to_string())?;
    let mut recording_data = state.recording_data.lock().map_err(|e| e.to_string())?;
    
    if *is_recording {
        return Err("Already recording".to_string());
    }
    
    // Set up output path
    let home_dir = dirs::home_dir().ok_or("Could not find home directory")?;
    let recordings_dir = home_dir.join("Documents").join("MeetingRecordings");
    std::fs::create_dir_all(&recordings_dir).map_err(|e| e.to_string())?;
    
    let timestamp = chrono::Utc::now().format("%Y%m%d_%H%M%S");
    let file_path = recordings_dir.join(format!("recording_{}.wav", timestamp));
    
    *output_path = Some(file_path.clone());
    *start_time = Some(chrono::Utc::now());
    *is_recording = true;
    recording_data.clear();
    
    // Start actual audio recording in a separate thread
    let recording_data_clone = state.recording_data.clone();
    let is_recording_clone = state.is_recording.clone();
    let whisper_context_clone = state.whisper_context.clone();
    let is_realtime_clone = state.is_realtime_enabled.clone();
    let chunk_size = state.chunk_size;
    
    thread::spawn(move || {
        if let Err(e) = start_audio_capture_with_realtime(
            recording_data_clone, 
            is_recording_clone,
            whisper_context_clone,
            is_realtime_clone,
            chunk_size
        ) {
            eprintln!("Audio capture error: {}", e);
        }
    });
    
    Ok(format!("Recording started: {}", file_path.display()))
}

fn start_audio_capture_with_realtime(
    recording_data: Arc<Mutex<Vec<f32>>>,
    is_recording: Arc<Mutex<bool>>,
    whisper_context: Arc<Mutex<Option<WhisperContext>>>,
    is_realtime_enabled: Arc<Mutex<bool>>,
    chunk_size: usize,
) -> Result<(), String> {
    use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
    
    let host = cpal::default_host();
    let target_sample_rate = 16000u32; // Whisper's preferred sample rate
    
    // Get microphone device
    let mic_device = host.default_input_device()
        .ok_or_else(|| "No microphone device available. Please check your microphone connection.".to_string())?;
    
    let mic_name = mic_device.name().unwrap_or_else(|_| "Unknown Microphone".to_string());
    println!("Using microphone: {}", mic_name);
    
    // Get microphone configuration
    let mic_config = mic_device.default_input_config()
        .map_err(|e| format!("Failed to get microphone config: {}. Please check microphone permissions.", e))?;
    
    println!("Microphone config: {:?}", mic_config);
    
    let mic_sample_rate = mic_config.sample_rate().0;
    let mic_channels = mic_config.channels();
    
    // Try to get system audio device (loopback)
    let system_device = host.output_devices()
        .map_err(|e| format!("Failed to enumerate output devices: {}", e))?
        .find(|device| {
            if let Ok(name) = device.name() {
                // Look for system audio/loopback devices
                name.to_lowercase().contains("loopback") || 
                name.to_lowercase().contains("system") ||
                name.to_lowercase().contains("stereo mix") ||
                name.to_lowercase().contains("what u hear")
            } else {
                false
            }
        });
    
    // If no dedicated loopback device, try to use default output as input (macOS specific)
    let system_device = system_device.or_else(|| {
        // On macOS, we might need to use a different approach
        host.input_devices().ok()?.find(|device| {
            if let Ok(name) = device.name() {
                name.to_lowercase().contains("soundflower") ||
                name.to_lowercase().contains("blackhole") ||
                name.to_lowercase().contains("loopback")
            } else {
                false
            }
        })
    });
    
    // Shared buffers for audio data
    let mic_buffer = Arc::new(Mutex::new(Vec::<f32>::new()));
    let system_buffer = Arc::new(Mutex::new(Vec::<f32>::new()));
    
    // Clone references for closures
    let mic_buffer_clone = mic_buffer.clone();
    let system_buffer_clone = system_buffer.clone();
    let is_recording_mic = is_recording.clone();
    let is_recording_system = is_recording.clone();
    let mic_name_clone = mic_name.clone();
    
    // Start microphone capture
    let mic_stream = match mic_config.sample_format() {
        cpal::SampleFormat::F32 => {
            mic_device.build_input_stream(
                &mic_config.into(),
                move |data: &[f32], _: &cpal::InputCallbackInfo| {
                    if let Ok(is_rec) = is_recording_mic.lock() {
                        if *is_rec {
                            if let Ok(mut buffer) = mic_buffer_clone.lock() {
                                // Convert to mono and resample if needed
                                let mono_data = convert_to_mono(data, mic_channels);
                                let resampled = if mic_sample_rate != target_sample_rate {
                                    resample_audio(&mono_data, mic_sample_rate, target_sample_rate)
                                } else {
                                    mono_data
                                };
                                buffer.extend_from_slice(&resampled);
                            }
                        }
                    }
                },
                move |err| eprintln!("Microphone error on '{}': {}", mic_name_clone, err),
                None,
            )
        }
        cpal::SampleFormat::I16 => {
            mic_device.build_input_stream(
                &mic_config.into(),
                move |data: &[i16], _: &cpal::InputCallbackInfo| {
                    if let Ok(is_rec) = is_recording_mic.lock() {
                        if *is_rec {
                            if let Ok(mut buffer) = mic_buffer_clone.lock() {
                                // Convert I16 to F32, then to mono and resample
                                let f32_data = convert_i16_to_f32(data);
                                let mono_data = convert_to_mono(&f32_data, mic_channels);
                                let resampled = if mic_sample_rate != target_sample_rate {
                                    resample_audio(&mono_data, mic_sample_rate, target_sample_rate)
                                } else {
                                    mono_data
                                };
                                buffer.extend_from_slice(&resampled);
                            }
                        }
                    }
                },
                move |err| eprintln!("Microphone error on '{}': {}", mic_name_clone, err),
                None,
            )
        }
        _ => return Err(format!("Unsupported microphone sample format: {:?}", mic_config.sample_format())),
    }.map_err(|e| format!("Failed to build microphone stream: {}", e))?;
    
    // Start system audio capture if available
    let system_stream = if let Some(sys_device) = system_device {
        let sys_name = sys_device.name().unwrap_or_else(|_| "Unknown System Audio".to_string());
        println!("Using system audio: {}", sys_name);
        
        let sys_config = sys_device.default_input_config()
            .map_err(|e| format!("Failed to get system audio config: {}", e))?;
        
        println!("System audio config: {:?}", sys_config);
        
        let sys_sample_rate = sys_config.sample_rate().0;
        let sys_channels = sys_config.channels();
        let sys_name_clone = sys_name.clone();
        
        let stream = match sys_config.sample_format() {
            cpal::SampleFormat::F32 => {
                sys_device.build_input_stream(
                    &sys_config.into(),
                    move |data: &[f32], _: &cpal::InputCallbackInfo| {
                        if let Ok(is_rec) = is_recording_system.lock() {
                            if *is_rec {
                                if let Ok(mut buffer) = system_buffer_clone.lock() {
                                    // Convert to mono and resample if needed
                                    let mono_data = convert_to_mono(data, sys_channels);
                                    let resampled = if sys_sample_rate != target_sample_rate {
                                        resample_audio(&mono_data, sys_sample_rate, target_sample_rate)
                                    } else {
                                        mono_data
                                    };
                                    buffer.extend_from_slice(&resampled);
                                }
                            }
                        }
                    },
                    move |err| eprintln!("System audio error on '{}': {}", sys_name_clone, err),
                    None,
                )
            }
            cpal::SampleFormat::I16 => {
                sys_device.build_input_stream(
                    &sys_config.into(),
                    move |data: &[i16], _: &cpal::InputCallbackInfo| {
                        if let Ok(is_rec) = is_recording_system.lock() {
                            if *is_rec {
                                if let Ok(mut buffer) = system_buffer_clone.lock() {
                                    // Convert I16 to F32, then to mono and resample
                                    let f32_data = convert_i16_to_f32(data);
                                    let mono_data = convert_to_mono(&f32_data, sys_channels);
                                    let resampled = if sys_sample_rate != target_sample_rate {
                                        resample_audio(&mono_data, sys_sample_rate, target_sample_rate)
                                    } else {
                                        mono_data
                                    };
                                    buffer.extend_from_slice(&resampled);
                                }
                            }
                        }
                    },
                    move |err| eprintln!("System audio error on '{}': {}", sys_name_clone, err),
                    None,
                )
            }
            _ => return Err(format!("Unsupported system audio sample format: {:?}", sys_config.sample_format())),
        }.map_err(|e| format!("Failed to build system audio stream: {}", e))?;
        
        Some(stream)
    } else {
        println!("No system audio device found. Recording microphone only.");
        println!("To record system audio on macOS, install BlackHole or Soundflower.");
        None
    };
    
    // Start streams
    mic_stream.play().map_err(|e| format!("Failed to start microphone stream: {}", e))?;
    if let Some(ref stream) = system_stream {
        stream.play().map_err(|e| format!("Failed to start system audio stream: {}", e))?;
    }
    
    // Audio mixing and processing thread
    let recording_data_clone = recording_data.clone();
    let is_recording_mixer = is_recording.clone();
    let mic_buffer_mixer = mic_buffer.clone();
    let system_buffer_mixer = system_buffer.clone();
    
    thread::spawn(move || {
        let mut last_mic_len = 0;
        let mut last_system_len = 0;
        
        loop {
            thread::sleep(Duration::from_millis(100)); // Mix every 100ms
            
            // Check if still recording
            if let Ok(is_rec) = is_recording_mixer.lock() {
                if !*is_rec {
                    break;
                }
            }
            
            // Get current audio data
            let (mic_data, system_data) = {
                let mic_guard = mic_buffer_mixer.lock().unwrap();
                let system_guard = system_buffer_mixer.lock().unwrap();
                
                let new_mic_data = if mic_guard.len() > last_mic_len {
                    mic_guard[last_mic_len..].to_vec()
                } else {
                    Vec::new()
                };
                
                let new_system_data = if system_guard.len() > last_system_len {
                    system_guard[last_system_len..].to_vec()
                } else {
                    Vec::new()
                };
                
                last_mic_len = mic_guard.len();
                last_system_len = system_guard.len();
                
                (new_mic_data, new_system_data)
            };
            
            // Mix audio streams if we have new data
            if !mic_data.is_empty() || !system_data.is_empty() {
                // Mix with appropriate gains (mic slightly louder for clarity)
                let mixed = mix_audio_streams(&mic_data, &system_data, 0.7, 0.5);
                
                // Add to main recording buffer
                if let Ok(mut recording) = recording_data_clone.lock() {
                    recording.extend_from_slice(&mixed);
                }
            }
        }
    });
    
    // Real-time transcription processing thread
    let recording_data_rt = recording_data.clone();
    let is_recording_rt = is_recording.clone();
    let whisper_context_rt = whisper_context.clone();
    let is_realtime_rt = is_realtime_enabled.clone();
    
    thread::spawn(move || {
        let mut last_processed = 0;
        
        loop {
            thread::sleep(Duration::from_secs(5)); // Check every 5 seconds
            
            // Check if still recording
            if let Ok(is_rec) = is_recording_rt.lock() {
                if !*is_rec {
                    break;
                }
            }
            
            // Check if real-time is enabled
            let realtime_enabled = if let Ok(rt) = is_realtime_rt.lock() {
                *rt
            } else {
                continue;
            };
            
            if !realtime_enabled {
                continue;
            }
            
            // Process new audio chunks
            if let Ok(recording) = recording_data_rt.lock() {
                let current_len = recording.len();
                
                // If we have enough new data for a chunk
                if current_len >= last_processed + chunk_size {
                    let chunk_end = last_processed + chunk_size;
                    let chunk: Vec<f32> = recording[last_processed..chunk_end].to_vec();
                    
                    // Transcribe chunk in background
                    let whisper_ctx = whisper_context_rt.clone();
                    thread::spawn(move || {
                        if let Ok(ctx_guard) = whisper_ctx.lock() {
                            if let Some(ref ctx) = *ctx_guard {
                                match transcribe_with_whisper(ctx, &chunk) {
                                    Ok(transcript) => {
                                        println!("Real-time transcript: {}", transcript);
                                        // TODO: Send to frontend via event
                                    }
                                    Err(e) => {
                                        eprintln!("Real-time transcription error: {}", e);
                                    }
                                }
                            }
                        }
                    });
                    
                    last_processed = chunk_end;
                }
            }
        }
    });
    
    // Keep the streams alive while recording
    loop {
        thread::sleep(Duration::from_millis(100));
        if let Ok(is_rec) = is_recording.lock() {
            if !*is_rec {
                break;
            }
        }
    }
    
    Ok(())
}

fn start_audio_capture(
    recording_data: Arc<Mutex<Vec<f32>>>,
    is_recording: Arc<Mutex<bool>>,
) -> Result<(), String> {
    // Fallback to simple audio capture without real-time features
    start_audio_capture_with_realtime(
        recording_data,
        is_recording,
        Arc::new(Mutex::new(None)),
        Arc::new(Mutex::new(false)),
        0
    )
}

#[tauri::command]
async fn stop_recording(state: State<'_, AudioState>) -> Result<String, String> {
    let mut is_recording = state.is_recording.lock().map_err(|e| e.to_string())?;
    let mut start_time = state.start_time.lock().map_err(|e| e.to_string())?;
    let output_path = state.output_path.lock().map_err(|e| e.to_string())?;
    let recording_data = state.recording_data.lock().map_err(|e| e.to_string())?;
    
    if !*is_recording {
        return Err("Not currently recording".to_string());
    }
    
    *is_recording = false;
    *start_time = None;
    
    // Save the recorded audio to file
    if let Some(path) = output_path.as_ref() {
        let spec = hound::WavSpec {
            channels: 1,
            sample_rate: 16000, // Whisper expects 16kHz
            bits_per_sample: 16,
            sample_format: hound::SampleFormat::Int,
        };
        
        let mut writer = hound::WavWriter::create(path, spec)
            .map_err(|e| format!("Failed to create WAV file: {}", e))?;
        
        for &sample in recording_data.iter() {
            let sample_i16 = (sample * i16::MAX as f32) as i16;
            writer.write_sample(sample_i16)
                .map_err(|e| format!("Failed to write sample: {}", e))?;
        }
        
        writer.finalize()
            .map_err(|e| format!("Failed to finalize WAV file: {}", e))?;
        
        Ok(format!("Recording stopped and saved: {}", path.display()))
    } else {
        Ok("Recording stopped".to_string())
    }
}

#[tauri::command]
async fn save_files(state: State<'_, AudioState>) -> Result<String, String> {
    let output_path = state.output_path.lock().map_err(|e| e.to_string())?;
    
    if let Some(path) = output_path.as_ref() {
        Ok(format!("Files saved to: {}", path.display()))
    } else {
        Err("No recording to save".to_string())
    }
}

#[tauri::command]
async fn save_transcript_to_file(transcript: String, filename: Option<String>) -> Result<String, String> {
    use std::fs;
    use std::io::Write;
    
    if transcript.trim().is_empty() {
        return Err("No transcript content to save".to_string());
    }
    
    // Create the output directory
    let home_dir = dirs::home_dir()
        .ok_or("Could not find home directory")?;
    let output_dir = home_dir.join("Documents").join("MeetingRecordings");
    
    fs::create_dir_all(&output_dir)
        .map_err(|e| format!("Failed to create output directory: {}", e))?;
    
    // Generate filename if not provided
    let file_name = filename.unwrap_or_else(|| {
        let now = chrono::Utc::now();
        format!("meeting_{}.txt", now.format("%Y-%m-%d_%H-%M-%S"))
    });
    
    let file_path = output_dir.join(&file_name);
    
    // Write transcript to file
    let mut file = fs::File::create(&file_path)
        .map_err(|e| format!("Failed to create transcript file: {}", e))?;
    
    // Add metadata header
    let now = chrono::Utc::now();
    let header = format!(
        "Meeting Transcript\nGenerated: {}\nFile: {}\n{}\n\n",
        now.format("%Y-%m-%d %H:%M:%S UTC"),
        file_name,
        "=".repeat(50)
    );
    
    file.write_all(header.as_bytes())
        .map_err(|e| format!("Failed to write header to transcript file: {}", e))?;
    
    file.write_all(transcript.as_bytes())
        .map_err(|e| format!("Failed to write transcript to file: {}", e))?;
    
    Ok(format!("Transcript saved to: {}", file_path.display()))
}

#[tauri::command]
fn greet(name: &str) -> String {
    format!("Hello, {}! You've been greeted from Rust!", name)
}

#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_opener::init())
        .manage(AudioState::default())
        .invoke_handler(tauri::generate_handler![
            start_recording, 
            stop_recording, 
            save_files,
            save_transcript_to_file,
            get_audio_devices,
            initialize_whisper,
            transcribe_audio,
            enable_realtime_transcription,
            disable_realtime_transcription,
            get_recording_status,
            greet
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
